version: '1.0'

services:
 test_gpu:
    image: nvidia/cuda:12.3.1-base-ubuntu20.04
    command: nvidia-smi
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    networks:
      - chris_evil_llms
 ollama:
    build:
      context: .
      dockerfile: Dockerfile.ollama
    volumes:
      - ollama:/root/.ollama
    ports:
      - 11434:11434
    networks:
      - chris_evil_llms
    healthcheck:
      test: ["CMD", "curl", "-f", "http://ollama:11434"]
      interval: 30s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

 chatgpt:
    build:
      context: .
      dockerfile: Dockerfile.chatgpt
    ports:
      - 3000:3000
    environment:
      - 'DEFAULT_MODEL=deepseek-coder:33b'
      - 'OLLAMA_HOST=http://ollama:11434'
    depends_on:
      - ollama
    networks:
      - chris_evil_llms

 curl:
    image: curlimages/curl:latest
    environment:
      MODELS: llama2-uncensored deepseek-coder:33b wizardcoder:33b
      OLLAMA_ENDPOINT: http://ollama:11434/api
    command: >
      /bin/sh -c "
        for model in $$MODELS; do
          if ! curl -s $$OLLAMA_ENDPOINT/tags | grep -wq $$model; then
            curl -X POST $$OLLAMA_ENDPOINT/pull -d '{\"name\": \"'$$model'\"}' && sleep 5
          fi
        done
      "
    restart: on-failure
    depends_on:
      - ollama
    networks:
      - chris_evil_llms


volumes:
 ollama:

networks:
 chris_evil_llms:
    driver: bridge
